{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faec3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ab2a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor(4., grad_fn=<PowBackward0>)\n",
      "Since requires_grad=True, PyTorch remembers how y was created (from squaring x).\n"
     ]
    }
   ],
   "source": [
    "#When we create a tensor with requires_grad=True, PyTorch will track all operations on it.\n",
    "\n",
    "\n",
    "# Create a tensor and track gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function: y = x^2\n",
    "y = x ** 2\n",
    "\n",
    "print(\"y:\", y)\n",
    "print('Since requires_grad=True, PyTorch remembers how y was created (from squaring x).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6aa98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx at x=2: tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "y.backward()   # compute gradient\n",
    "print(\"dy/dx at x=2:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9873c9",
   "metadata": {},
   "source": [
    "## 1. Requires_grad\n",
    "âœ… Flow:\n",
    "\n",
    "If requires_grad=True â†’ tensor participates in autograd.\n",
    "\n",
    "If False â†’ tensor is treated as constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c94eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)  # track gradients\n",
    "y = torch.tensor(3.0, requires_grad=False) # no gradient tracking\n",
    "\n",
    "print(x.requires_grad)  # True\n",
    "print(y.requires_grad)  # False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4efb50",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 2. .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = x**2 + 1  # function\n",
    "y.backward()         # compute gradient\n",
    "\n",
    "print(x.grad)  # dy/dx = 2x  = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c024962",
   "metadata": {},
   "source": [
    "âœ… Flow:\n",
    "\n",
    "Build computation graph â†’ Call .backward() â†’ Autograd applies chain rule â†’ Store result in .grad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfe1d0",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 3. .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7a32ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3   # y = x^3\n",
    "y.backward()\n",
    "print(x.grad)  # dy/dx = 3x^2 = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487064e6",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 4. .zero_grad()\n",
    "Gradients accumulate in PyTorch by default. So before each new backward pass, we usually reset gradients to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0a6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first backward: tensor(3.)\n",
      "After second backward: tensor(7.)\n",
      "After zero_grad and backward: tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First pass\n",
    "y1 = w * 3\n",
    "y1.backward()\n",
    "print(\"After first backward:\", w.grad)  # 3\n",
    "\n",
    "# Second pass without zeroing\n",
    "y2 = w * 4\n",
    "y2.backward()\n",
    "print(\"After second backward:\", w.grad)  # 3 + 4 = 7 (accumulated!)\n",
    "\n",
    "# Reset gradients\n",
    "w.grad.zero_()\n",
    "y3 = w * 5\n",
    "y3.backward()\n",
    "print(\"After zero_grad and backward:\", w.grad)  # 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16a3b7",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 5. .detach()\n",
    "âœ… Flow:\n",
    "\n",
    "detach() cuts off the history â†’ gradients stop here.\n",
    "\n",
    "Useful in GANs, RNNs, or when freezing part of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x * 2       # tracked\n",
    "z = y.detach()  # not tracked\n",
    "\n",
    "print(y.requires_grad)  \n",
    "print(z.requires_grad)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ec5d3",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 6. torch.no_grad()\n",
    "âœ… Flow:\n",
    "\n",
    "Unlike .detach(), which applies to one tensor â†’ no_grad() disables gradient tracking for all operations inside the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5eb4b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2   # no graph built\n",
    "\n",
    "print(y.requires_grad) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c26c7a",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Putting It All Together (Mini Training Example)\n",
    "âœ… Flow:\n",
    "\n",
    "requires_grad=True â†’ track w.\n",
    "\n",
    ".backward() â†’ compute gradient.\n",
    "\n",
    ".grad â†’ access gradient.\n",
    "\n",
    "torch.no_grad() â†’ update parameters safely (no tracking).\n",
    "\n",
    ".zero_grad() â†’ reset for next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "581a1cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Grad: -12.0\n",
      "Epoch 2, Grad: -2.3999996185302734\n",
      "Epoch 3, Grad: -0.4799995422363281\n"
     ]
    }
   ],
   "source": [
    "# A simple linear regression example\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # Forward pass\n",
    "    x = torch.tensor(2.0)\n",
    "    target = torch.tensor(5.0)\n",
    "    y = w * x\n",
    "    loss = (y - target) ** 2  # MSE\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()      # compute gradients\n",
    "    print(f\"Epoch {epoch+1}, Grad: {w.grad.item()}\")\n",
    "\n",
    "    # Update weight\n",
    "    with torch.no_grad():\n",
    "        w -= 0.1 * w.grad  # gradient descent\n",
    "\n",
    "    # Reset gradients\n",
    "    w.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14285d",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Final Summary\n",
    "\n",
    "requires_grad â†’ Tell PyTorch to track gradients for a tensor.\n",
    "\n",
    ".backward() â†’ Compute gradients (autograd).\n",
    "\n",
    ".grad â†’ Stores gradient result.\n",
    "\n",
    ".zero_grad() â†’ Reset gradients (important in loops).\n",
    "\n",
    ".detach() â†’ Get a copy of tensor without gradient tracking.\n",
    "\n",
    "torch.no_grad() â†’ Disable gradient tracking inside a block (used in inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a9efc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " tensor([[2., 3.],\n",
      "        [1., 4.]], requires_grad=True)\n",
      "Matrix B:\n",
      " tensor([[1., 2.],\n",
      "        [3., 1.]], requires_grad=True)\n",
      "Matrix C (A*B):\n",
      " tensor([[11.,  7.],\n",
      "        [13.,  6.]], grad_fn=<MmBackward0>)\n",
      "Loss (sum of C): 37.0\n",
      "\n",
      "Gradients:\n",
      "dLoss/dA:\n",
      " tensor([[3., 4.],\n",
      "        [3., 4.]])\n",
      "dLoss/dB:\n",
      " tensor([[3., 3.],\n",
      "        [7., 7.]])\n",
      "\n",
      "After zero_grad():\n",
      "A.grad:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "B.grad:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "Detached A (no gradient tracking):\n",
      " tensor([[2., 3.],\n",
      "        [1., 4.]])\n",
      "\n",
      "C computed under no_grad:\n",
      " tensor([[11.,  7.],\n",
      "        [13.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create input matrices with requires_grad=True\n",
    "A = torch.tensor([[2.0, 3.0],\n",
    "                  [1.0, 4.0]], requires_grad=True)\n",
    "\n",
    "B = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 1.0]], requires_grad=True)\n",
    "\n",
    "# Step 2: Forward pass (simple operation)\n",
    "C = torch.matmul(A, B)   # Matrix multiplication\n",
    "loss = C.sum()           # Scalar loss (required for backward)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Matrix B:\\n\", B)\n",
    "print(\"Matrix C (A*B):\\n\", C)\n",
    "print(\"Loss (sum of C):\", loss.item())\n",
    "\n",
    "# Step 3: Backward pass\n",
    "loss.backward()   # Compute gradients\n",
    "\n",
    "# Step 4: Check gradients\n",
    "print(\"\\nGradients:\")\n",
    "print(\"dLoss/dA:\\n\", A.grad)\n",
    "print(\"dLoss/dB:\\n\", B.grad)\n",
    "\n",
    "# Step 5: Zero gradients before next backward (important in training loops)\n",
    "A.grad.zero_()\n",
    "B.grad.zero_()\n",
    "print(\"\\nAfter zero_grad():\")\n",
    "print(\"A.grad:\\n\", A.grad)\n",
    "print(\"B.grad:\\n\", B.grad)\n",
    "\n",
    "# Step 6: Detach (stop gradient tracking for a tensor)\n",
    "A_detached = A.detach()\n",
    "print(\"\\nDetached A (no gradient tracking):\\n\", A_detached)\n",
    "\n",
    "# Step 7: No grad context (useful for inference)\n",
    "with torch.no_grad():\n",
    "    C_no_grad = torch.matmul(A, B)\n",
    "print(\"\\nC computed under no_grad:\\n\", C_no_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a3c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
